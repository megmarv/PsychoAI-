{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1llkMqmFYsknOsm8BIV6DND0Q0eocmhNw",
      "authorship_tag": "ABX9TyNn2cxZZ84iQ9spau9ze6T2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megmarv/PsychoAI-/blob/Emotion-Identification2/preprocessing_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijzXCW463OjW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca80dfbe-53e5-45da-df62-cf0660b213f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preprocessing and augmentation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Images: 100%|██████████| 26171/26171 [17:15<00:00, 25.28img/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Preprocessing & augmentation complete! Balanced dataset saved in /content/drive/MyDrive/CNN model/preprocessed_dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tqdm import tqdm  # Progress bar\n",
        "\n",
        "# Define paths\n",
        "data_dir = \"/content/drive/MyDrive/CNN model/new_dataset_for_custom_model\"\n",
        "output_dir = \"/content/drive/MyDrive/CNN model/preprocessed_dataset\"\n",
        "emotions = [\"anger\", \"happy\", \"sad\", \"neutral\", \"surprise\", \"fear\", \"disgust\"]\n",
        "target_size = (224, 224)  # Match ResNet50V2 input size\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "for emotion in emotions:\n",
        "    os.makedirs(os.path.join(output_dir, emotion), exist_ok=True)\n",
        "\n",
        "# Augmentation settings\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.8, 1.2]\n",
        ")\n",
        "\n",
        "# Step 1: Count images per class\n",
        "class_counts = {emotion: len(os.listdir(os.path.join(data_dir, emotion))) for emotion in emotions}\n",
        "max_count = max(class_counts.values())  # Find the largest class count\n",
        "total_images = sum(class_counts.values())  # Total number of images before balancing\n",
        "\n",
        "# Step 2: Preprocess & Augment for balancing\n",
        "def preprocess_and_augment(image_path, target_size, save_dir, image_name, augment_needed):\n",
        "    \"\"\"Preprocesses an image and applies augmentation if needed.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    image = cv2.resize(image, target_size)  # Resize to 224x224\n",
        "    normalized_image = image / 255.0  # Normalize to [0,1]\n",
        "\n",
        "    # Save original preprocessed image\n",
        "    save_path = os.path.join(save_dir, image_name)\n",
        "    cv2.imwrite(save_path, (normalized_image * 255).astype(np.uint8))\n",
        "\n",
        "    # Apply augmentation if needed\n",
        "    if augment_needed > 0:\n",
        "        image_expanded = np.expand_dims(normalized_image, axis=0)\n",
        "        augmented_images = datagen.flow(image_expanded, batch_size=1)\n",
        "\n",
        "        for i in range(augment_needed):\n",
        "            aug_image = next(augmented_images)[0]  # Get the augmented image\n",
        "            aug_image = (aug_image * 255).astype(np.uint8)  # Convert back to 0-255 range\n",
        "            aug_save_path = os.path.join(save_dir, f\"aug_{i}_{image_name}\")\n",
        "            cv2.imwrite(aug_save_path, aug_image)\n",
        "\n",
        "# Step 3: Process dataset with balancing and visualize progress\n",
        "print(\"Starting preprocessing and augmentation...\")\n",
        "\n",
        "total_processed = 0\n",
        "with tqdm(total=total_images, desc=\"Processing Images\", unit=\"img\") as pbar:\n",
        "    for emotion in emotions:\n",
        "        emotion_dir = os.path.join(data_dir, emotion)\n",
        "        output_emotion_dir = os.path.join(output_dir, emotion)\n",
        "\n",
        "        image_files = os.listdir(emotion_dir)\n",
        "        augment_needed_per_image = max(0, max_count - len(image_files)) // len(image_files)  # Augmentation per image\n",
        "\n",
        "        for image_name in image_files:\n",
        "            image_path = os.path.join(emotion_dir, image_name)\n",
        "            preprocess_and_augment(image_path, target_size, output_emotion_dir, image_name, augment_needed_per_image)\n",
        "\n",
        "            total_processed += 1\n",
        "            pbar.update(1)  # Update progress bar\n",
        "\n",
        "print(\"✅ Preprocessing & augmentation complete! Balanced dataset saved in\", output_dir)"
      ]
    }
  ]
}