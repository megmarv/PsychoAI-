{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megmarv/PsychoAI-/blob/Image-Retrieval/Custom_Made_Face_Recognition_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python tensorflow numpy matplotlib\n"
      ],
      "metadata": {
        "id": "d6E0TWJNjOiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Face Extraction and Classification Code"
      ],
      "metadata": {
        "id": "PZu3YlwpjSJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Preparation"
      ],
      "metadata": {
        "id": "y7OYydSSl5MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to load an image and its corresponding bounding boxes\n",
        "def load_image_and_annotations(image_path, annotation_path, img_size=(224, 224)):\n",
        "    # Load image\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, img_size)\n",
        "    img = img.astype('float32') / 255.0  # Normalize image\n",
        "\n",
        "    # Load annotations (bounding boxes)\n",
        "    bboxes = []\n",
        "    with open(annotation_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            bbox = line.strip().split()\n",
        "            if len(bbox) >= 4:\n",
        "                bbox = [float(coord) for coord in bbox[:4]]\n",
        "                bboxes.append(bbox)\n",
        "\n",
        "    return img, np.array(bboxes)\n",
        "\n",
        "# Function to load dataset from AFW folder\n",
        "def load_afw_dataset(image_dir, annotation_dir, img_size=(224, 224)):\n",
        "    images = []\n",
        "    bboxes = []\n",
        "\n",
        "    for img_name in os.listdir(image_dir):\n",
        "        if img_name.endswith('.jpg'):\n",
        "            img_path = os.path.join(image_dir, img_name)\n",
        "            annotation_path = os.path.join(annotation_dir, img_name.replace('.jpg', '.txt'))\n",
        "\n",
        "            # Load image and annotations\n",
        "            img, bbox = load_image_and_annotations(img_path, annotation_path, img_size)\n",
        "\n",
        "            images.append(img)\n",
        "            bboxes.append(bbox)\n",
        "\n",
        "    images = np.array(images)\n",
        "    bboxes = np.array(bboxes)\n",
        "\n",
        "    return images, bboxes\n",
        "\n",
        "# Example usage\n",
        "dataset_dir = 'path_to_afw_dataset'  # Update with path to your AFW dataset\n",
        "image_dir = os.path.join(dataset_dir, 'images')\n",
        "annotation_dir = os.path.join(dataset_dir, 'annotations')\n",
        "\n",
        "# Load the dataset\n",
        "images, bboxes = load_afw_dataset(image_dir, annotation_dir)\n",
        "\n",
        "print(f\"Loaded {len(images)} images with bounding boxes.\")\n"
      ],
      "metadata": {
        "id": "ZfBupwvFl8Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Creation"
      ],
      "metadata": {
        "id": "OX8aZ6vil2Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Custom CNN for Face Detection (Bounding Box Prediction)\n",
        "def build_face_detection_model(input_shape):\n",
        "    model = models.Sequential([\n",
        "        # Feature Extraction Layers\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Bounding Box Prediction Layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(4, activation='sigmoid')  # 4 values: x, y, width, height\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile model\n",
        "def compile_model(model):\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Model summary\n",
        "input_shape = (224, 224, 3)  # Resize image to (224, 224)\n",
        "model = build_face_detection_model(input_shape)\n",
        "model = compile_model(model)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "9o2Re8u1jPYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "CHLY4Db9l-vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, bboxes, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "sXOcbGy1mC1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Testing and Evaluation"
      ],
      "metadata": {
        "id": "1yVanRVMmDrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test data\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "\n",
        "# Test the model on a single image\n",
        "def predict_face(model, image_path, target_size=(224, 224)):\n",
        "    img = cv2.imread(image_path)\n",
        "    img_resized = cv2.resize(img, target_size)\n",
        "    img_normalized = img_resized.astype('float32') / 255.0\n",
        "    img_input = np.expand_dims(img_normalized, axis=0)  # Add batch dimension\n",
        "\n",
        "    # Predict bounding box for face\n",
        "    bbox = model.predict(img_input)[0]\n",
        "\n",
        "    # Scale bounding box back to original image size\n",
        "    h, w, _ = img.shape\n",
        "    bbox = [int(coord * dimension) for coord, dimension in zip(bbox, [w, h, w, h])]\n",
        "\n",
        "    # Extract face using the predicted bounding box\n",
        "    x, y, w, h = bbox\n",
        "    face = img[y:y+h, x:x+w]\n",
        "\n",
        "    return face, bbox\n",
        "\n",
        "# Example usage: Test on a single image\n",
        "image_path = 'path_to_image.jpg'\n",
        "face, bbox = predict_face(model, image_path)\n",
        "\n",
        "# Show the extracted face\n",
        "if face is not None:\n",
        "    cv2.imshow('Extracted Face', face)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "WWEfZTejmGMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}