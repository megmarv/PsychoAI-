{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1rsbz1FLRAOVVRUHBM8XCBF1QK9O0meLf",
      "authorship_tag": "ABX9TyNrla7EJeYQ7BlSYePXK5wU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megmarv/PsychoAI-/blob/Image-Retrieval/FaceExtraction_customModel_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WiL0tboIzCb5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import dlib\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directory if not exists\n",
        "output_folder = \"extracted_faces\"\n",
        "os.makedirs(output_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "FpHWlCWtNXf3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load image\n",
        "def detect_and_save_faces(image_path):\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces = detector(gray)\n",
        "\n",
        "    face_images = []\n",
        "    for i, face in enumerate(faces):\n",
        "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
        "        face_img = gray[y:y+h, x:x+w]\n",
        "        face_resized = cv2.resize(face_img, (100, 100))  # Resizing to the required input size\n",
        "\n",
        "        # Ensure the face image is in the correct format (grayscale, 1 channel)\n",
        "        face_resized = np.expand_dims(face_resized, axis=-1)  # Adding the channel dimension\n",
        "\n",
        "        face_path = os.path.join(output_folder, f\"face_{i}.jpg\")\n",
        "        cv2.imwrite(face_path, face_resized)  # Save the face as grayscale image\n",
        "        face_images.append(face_resized)\n",
        "\n",
        "    return face_images"
      ],
      "metadata": {
        "id": "-K-OC8HdzUWk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a simple CNN on extracted faces\n",
        "def build_face_recognition_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 1)),\n",
        "        MaxPooling2D(2,2),\n",
        "        Conv2D(64, (3,3), activation='relu'),\n",
        "        MaxPooling2D(2,2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(2, activation='softmax')  # Assuming two classes for demo\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "tGzPT2D4zcyL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for training\n",
        "def prepare_data():\n",
        "    face_images = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for filename in os.listdir(output_folder):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):  # Check if it's an image file\n",
        "            face_path = os.path.join(output_folder, filename)\n",
        "            face_img = cv2.imread(face_path, cv2.IMREAD_GRAYSCALE)\n",
        "            face_resized = cv2.resize(face_img, (100, 100))  # Resize to model input size\n",
        "            face_resized = np.expand_dims(face_resized, axis=-1)  # Add channel dimension (grayscale)\n",
        "\n",
        "            face_images.append(face_resized)\n",
        "            # Here we assume labels for two classes (0 and 1), you need to update based on your use case\n",
        "            label = 0  # Update with appropriate label for your case\n",
        "            labels.append(label)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    face_images = np.array(face_images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(face_images, labels, test_size=0.2, random_state=42)\n",
        "    return train_paths, val_paths, train_labels, val_labels\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Change this to your actual group photos directory\n",
        "    group_photos_dir = \"/content/drive/MyDrive/Group-Photos\"\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for filename in os.listdir(group_photos_dir):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):  # Check if it's an image file\n",
        "            image_path = os.path.join(group_photos_dir, filename)\n",
        "            detect_and_save_faces(image_path)\n",
        "\n",
        "    # Prepare data for training and validation\n",
        "    train_data, val_data, train_labels, val_labels = prepare_data()\n",
        "\n",
        "    # Initialize and train the model\n",
        "    model = build_face_recognition_model()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=10)\n",
        "\n",
        "    # Save the model\n",
        "    model.save(\"face_recognition_model.h5\")\n",
        "    print(\"Model trained and saved successfully.\")"
      ],
      "metadata": {
        "id": "BzTbP6SMzfqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea84d44-bc6e-4831-fe2e-1e75ea227745"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 18.9935 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 563ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 888ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 896ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved successfully.\n"
          ]
        }
      ]
    }
  ]
}