{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11iKyrISUyCgPq2r5bztxJ17uGuscs6jh",
      "authorship_tag": "ABX9TyMO/1E4XBRTYIgmRvCy6JNr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megmarv/PsychoAI-/blob/Image-Retrieval/FaceExtraction_customModel_01_updated1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K"
      ],
      "metadata": {
        "id": "dJ2Y2VynUQA4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory for face images\n",
        "output_folder = \"extracted_faces\"\n",
        "os.makedirs(output_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "va5wLY0tURxa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to detect and save faces\n",
        "def detect_and_save_faces(image_path):\n",
        "    # Initialize dlib face detector\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "\n",
        "    face_images = []\n",
        "    for i, face in enumerate(faces):\n",
        "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
        "        face_img = gray[y:y+h, x:x+w]\n",
        "        face_resized = cv2.resize(face_img, (100, 100))\n",
        "        face_path = os.path.join(output_folder, f\"face_{i}.jpg\")\n",
        "        cv2.imwrite(face_path, face_resized)\n",
        "        face_images.append(face_resized)\n",
        "\n",
        "    return face_images\n"
      ],
      "metadata": {
        "id": "O5i4UafxUUJ0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset for training and validation\n",
        "def prepare_data():\n",
        "    face_images = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop through all files in the directory\n",
        "    for filename in os.listdir(output_folder):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):  # Check if it's an image file\n",
        "            face_path = os.path.join(output_folder, filename)\n",
        "            face_img = cv2.imread(face_path, cv2.IMREAD_GRAYSCALE)\n",
        "            face_resized = cv2.resize(face_img, (100, 100))  # Resize to model input size\n",
        "            face_resized = np.expand_dims(face_resized, axis=-1)  # Add channel dimension (grayscale)\n",
        "\n",
        "            face_images.append(face_resized)\n",
        "            # Here, assign labels based on your use case; for now, we'll assume a label 0\n",
        "            label = 0  # Update with appropriate label for your case\n",
        "            labels.append(label)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    face_images = np.array(face_images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    train_data, val_data, train_labels, val_labels = train_test_split(face_images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Normalize the images\n",
        "    train_data = train_data / 255.0\n",
        "    val_data = val_data / 255.0\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    train_labels = to_categorical(train_labels, num_classes=2)  # Assuming two classes\n",
        "    val_labels = to_categorical(val_labels, num_classes=2)\n",
        "\n",
        "    return train_data, val_data, train_labels, val_labels\n"
      ],
      "metadata": {
        "id": "2257DHTOUd3M"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 score metric using TensorFlow\n",
        "def f1_score_metric(y_true, y_pred):\n",
        "    # Convert predictions and true labels to binary format (0 or 1)\n",
        "    y_true = K.argmax(y_true, axis=-1)\n",
        "    y_pred = K.argmax(y_pred, axis=-1)\n",
        "\n",
        "    # Calculate precision, recall, and F1 score using TensorFlow operations\n",
        "    true_positives = K.sum(K.cast(K.equal(y_true, 1) & K.equal(y_pred, 1), K.floatx()))\n",
        "    false_positives = K.sum(K.cast(K.equal(y_true, 0) & K.equal(y_pred, 1), K.floatx()))\n",
        "    false_negatives = K.sum(K.cast(K.equal(y_true, 1) & K.equal(y_pred, 0), K.floatx()))\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives + K.epsilon())\n",
        "    recall = true_positives / (true_positives + false_negatives + K.epsilon())\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "\n",
        "    return f1_score"
      ],
      "metadata": {
        "id": "10YB0tIkUiKC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build CNN model\n",
        "def build_face_recognition_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 1)),  # Grayscale images have 1 channel\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(2, activation='softmax')  # Assuming binary classification (2 classes)\n",
        "    ])\n",
        "\n",
        "    # Compile the model with F1 score as a custom metric\n",
        "    model.compile(\n",
        "        optimizer=Adam(),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', f1_score_metric]  # Add F1 score metric\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Specify the directory for group photos\n",
        "    group_photos_dir = \"/content/drive/MyDrive/Group-Photos\"\n",
        "\n",
        "    # Detect and save faces from the group photos\n",
        "    for filename in os.listdir(group_photos_dir):\n",
        "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(group_photos_dir, filename)\n",
        "            detect_and_save_faces(image_path)\n",
        "\n",
        "    # Prepare the dataset for training and validation\n",
        "    train_data, val_data, train_labels, val_labels = prepare_data()\n",
        "\n",
        "    # Build and compile the model\n",
        "    model = build_face_recognition_model()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=10)\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save(\"face_recognition_model.h5\")\n",
        "    print(\"Model trained and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2GZzdQmUEJm",
        "outputId": "85d9db1d-703c-4eaa-e808-a78adecbeb64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 1.0000 - f1_score_metric: 0.0000e+00 - loss: 0.6556 - val_accuracy: 1.0000 - val_f1_score_metric: 0.0000e+00 - val_loss: 8.6985e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 334ms/step - accuracy: 1.0000 - f1_score_metric: 0.0000e+00 - loss: 0.0187 - val_accuracy: 1.0000 - val_f1_score_metric: 0.0000e+00 - val_loss: 3.5763e-07\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 1.0000 - f1_score_metric: 0.0000e+00 - loss: 2.8116e-04 - val_accuracy: 1.0000 - val_f1_score_metric: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - accuracy: 1.0000 - f1_score_metric: 0.0000e+00 - loss: 3.6359e-06 - val_accuracy: 1.0000 - val_f1_score_metric: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 1.0000 - f1_score_metric: 0.0000e+00 - loss: 5.9605e-08 - val_accuracy: 1.0000 - val_f1_score_metric: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 1.0000 - f1_score_metric: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_f1_score_metric: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 1.0000 - f1_score_metric: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_f1_score_metric: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 1.0000 - f1_score_metric: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_f1_score_metric: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324ms/step - accuracy: 1.0000 - f1_score_metric: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_f1_score_metric: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 1.0000 - f1_score_metric: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_f1_score_metric: 0.0000e+00 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved successfully.\n"
          ]
        }
      ]
    }
  ]
}