{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsv7puPo+QZLKLhqNCMLRS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megmarv/PsychoAI-/blob/Emotion-Identification3/Custom_CNN_HybridDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "B12xA3LpK0Jn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EssNGjaGKojY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset directory\n",
        "DATASET_DIR = 'Refined_Hybrid_Dataset'  # Update this path\n",
        "\n",
        "# Set image size and batch size\n",
        "IMG_SIZE = (48, 48)\n",
        "BATCH_SIZE = 64  # Keep it 64 for batch optimization\n",
        "\n",
        "# Remove augmentation since data is balanced\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)  # No augmentation for validation\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    os.path.join(DATASET_DIR, \"train\"),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    color_mode=\"grayscale\",\n",
        "    class_mode=\"categorical\"\n",
        ")\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(\n",
        "    os.path.join(DATASET_DIR, \"test\"),\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    color_mode=\"grayscale\",\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False  # Do not shuffle for confusion matrix consistency\n",
        ")\n",
        "\n",
        "# Print dataset details\n",
        "print(f\"Training samples: {train_data.samples}, Classes: {train_data.num_classes}\")\n",
        "print(f\"Test samples: {test_data.samples}, Classes: {test_data.num_classes}\")"
      ],
      "metadata": {
        "id": "vxOykJMNK7vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_files_in_subdirs(directory, set_name):\n",
        "    counts = {}\n",
        "    for item in os.listdir(directory):\n",
        "        item_path = os.path.join(directory, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            counts[item] = len(os.listdir(item_path))\n",
        "    df = pd.DataFrame(counts, index=[set_name])\n",
        "    return df\n",
        "\n",
        "train_dir = 'Refined_Hybrid_Dataset/train'\n",
        "test_dir = 'Refined_Hybrid_Dataset/test'\n",
        "\n",
        "train_count = count_files_in_subdirs(train_dir, 'train')\n",
        "print(train_count)\n",
        "\n",
        "test_count = count_files_in_subdirs(test_dir, 'test')\n",
        "print(test_count)"
      ],
      "metadata": {
        "id": "IxJUM2vMK--n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_count.transpose().plot(kind='bar')"
      ],
      "metadata": {
        "id": "Nrcivbv3LCIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_count.transpose().plot(kind='bar')"
      ],
      "metadata": {
        "id": "MVQvj8oFLEQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(48,48,1)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(256, (3,3), activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Conv2D(512, (3,3), activation='relu', padding='same'),  # NEW LAYER\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(2,2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),  # Increase dropout\n",
        "    Dense(256, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "    Dense(7, activation='softmax')  # 7 emotion classes\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "6Lvmy30bLGOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4)"
      ],
      "metadata": {
        "id": "0Yd0zPGuLIaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=test_data,\n",
        "    epochs=40,  # Keep or adjust as needed\n",
        "    callbacks=[early_stop, reduce_lr]  # Keep callbacks for efficiency\n",
        ")"
      ],
      "metadata": {
        "id": "ti0YI862LKvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting Accuracy and Loss over Epochs\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plotting Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy over Epochs\")\n",
        "\n",
        "# Plotting Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.title(\"Loss over Epochs\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W_k0OE9jLMbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict on validation data (test_data)\n",
        "y_pred = model.predict(test_data)  # Using test_data (ImageDataGenerator for validation)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Get true labels from test_data (using the flow_from_directory)\n",
        "y_true_classes = test_data.classes  # Extract true labels from the 'classes' attribute of the generator\n",
        "\n",
        "# Generate Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, cmap=\"Blues\", fmt='g',\n",
        "            xticklabels=test_data.class_indices.keys(),  # Using class labels from the generator\n",
        "            yticklabels=test_data.class_indices.keys())  # Using class labels from the generator\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-EWPiAgpLOaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Get predictions\n",
        "y_pred_prob = model.predict(test_data, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "# Get true labels\n",
        "y_true_classes = test_data.classes  # Extract true labels\n",
        "class_labels = list(test_data.class_indices.keys())  # Get class names\n",
        "\n",
        "# Generate Classification Report\n",
        "report = classification_report(y_true_classes, y_pred_classes, target_names=class_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "ZBNBD1jvLSQP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}