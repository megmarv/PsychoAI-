{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUoR28cccNbhtSX/unaa2l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/megmarv/PsychoAI-/blob/Emotion-Identification2/custom_Model_Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NH4Hn8M53-yi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import imghdr\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from tensorflow.keras import layers, models, regularizers, optimizers\n",
        "from tensorflow.keras.applications import VGG16, ResNet50V2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense, Activation, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load preprocessed data from .npy files\n",
        "X_train = np.load(\"X_train.npy\")\n",
        "y_train = np.load(\"y_train.npy\")\n",
        "\n",
        "# Define the list of acceptable image extensions (for reference, not used here)\n",
        "image_exts = ['jpeg', 'jpg', 'png']\n",
        "\n",
        "# Function to validate and preprocess images (if needed)\n",
        "def validate_and_preprocess(image):\n",
        "    \"\"\"\n",
        "    Validate and preprocess an image (if needed).\n",
        "    This function can be used to perform additional checks or transformations.\n",
        "    \"\"\"\n",
        "    # Example: Check if the image is valid (not empty)\n",
        "    if image is None or image.size == 0:\n",
        "        print(\"Invalid image detected.\")\n",
        "        return None\n",
        "\n",
        "    # Example: Convert to grayscale if not already\n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:  # Check if RGB\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Iterate through the preprocessed images and labels\n",
        "for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
        "    try:\n",
        "        # Validate and preprocess the image (if needed)\n",
        "        processed_image = validate_and_preprocess(image)\n",
        "\n",
        "        if processed_image is None:\n",
        "            print(f\"Skipping invalid image at index {i}.\")\n",
        "            continue\n",
        "\n",
        "        # Update the image in the dataset (if preprocessing was applied)\n",
        "        X_train[i] = processed_image\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions that occur during processing\n",
        "        print(f\"Issue with image at index {i}. Error: {e}\")\n",
        "\n",
        "# Save the updated dataset (if any changes were made)\n",
        "np.save(\"X_train_validated.npy\", X_train)\n",
        "np.save(\"y_train_validated.npy\", y_train)\n",
        "\n",
        "print(\"Validation and preprocessing complete!\")"
      ],
      "metadata": {
        "id": "ytYQUdW74Rj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define a function to count the number of samples in preprocessed .npy files\n",
        "def count_samples_in_npy(data_dir, set_name):\n",
        "    \"\"\"\n",
        "    Count the number of samples in preprocessed .npy files for each class.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory containing the .npy files.\n",
        "        set_name (str): Name of the dataset (e.g., 'train' or 'test').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with the counts of samples for each class.\n",
        "    \"\"\"\n",
        "    # Initialize an empty dictionary to hold the count of samples for each class\n",
        "    counts = {}\n",
        "\n",
        "    # Load the preprocessed data\n",
        "    X = np.load(os.path.join(data_dir, f\"X_{set_name}.npy\"))\n",
        "    y = np.load(os.path.join(data_dir, f\"y_{set_name}.npy\"))\n",
        "\n",
        "    # Get unique classes and their counts\n",
        "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
        "\n",
        "    # Map class indices to class names (if you have a label encoder or class names)\n",
        "    # Assuming you have a list of class names in the same order as the encoded labels\n",
        "    class_names = [\"anger\", \"happy\", \"sad\", \"neutral\", \"surprise\", \"fear\", \"disgust\"]\n",
        "\n",
        "    # Populate the counts dictionary\n",
        "    for class_idx, count in zip(unique_classes, class_counts):\n",
        "        class_name = class_names[class_idx]  # Map class index to class name\n",
        "        counts[class_name] = count\n",
        "\n",
        "    # Convert the counts dictionary to a DataFrame for easy viewing and analysis\n",
        "    df = pd.DataFrame(counts, index=[set_name])\n",
        "    return df\n",
        "\n",
        "# Paths to the directories containing the preprocessed .npy files\n",
        "train_data_dir = 'C:/Users/LENOVO/PyCharmMiscProject' # Directory containing X_train.npy and y_train.npy\n",
        "test_data_dir = 'C:/Users/LENOVO/PyCharmMiscProject'# Directory containing X_test.npy and y_test.npy\n",
        "\n",
        "# Count the samples in the training dataset and print the result\n",
        "train_count = count_samples_in_npy(train_data_dir, 'train')\n",
        "print(train_count)\n",
        "\n",
        "# Count the samples in the testing dataset and print the result\n",
        "test_count = count_samples_in_npy(test_data_dir, 'test')\n",
        "print(test_count)"
      ],
      "metadata": {
        "id": "WHiiGg8a4T9p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}